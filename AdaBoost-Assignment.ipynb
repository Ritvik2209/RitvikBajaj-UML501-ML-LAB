{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d5af66",
   "metadata": {},
   "source": [
    "## Q1: SMS Spam Collection Dataset — AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dfd97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A: Data Preprocessing & Exploration\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "df = pd.read_csv('spam.csv')  # Update with actual path\n",
    "df['label'] = df['label'].map({'spam': 1, 'ham': 0})\n",
    "# Text preprocessing\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z ]', '', text)\n",
    "    text = ' '.join([w for w in text.split() if w not in stop_words])\n",
    "    return text\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pd.Series(y).value_counts()  # Show class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e16989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part B: Weak Learner Baseline (Decision Stump)\n",
    "stump = DecisionTreeClassifier(max_depth=1)\n",
    "stump.fit(X_train, y_train)\n",
    "y_pred_train = stump.predict(X_train)\n",
    "y_pred_test = stump.predict(X_test)\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "conf = confusion_matrix(y_test, y_pred_test)\n",
    "acc_train, acc_test, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293271f2",
   "metadata": {},
   "source": [
    "## Part C: Manual AdaBoost (T = 15)\n",
    "# (Pseudocode and structure for manual AdaBoost with weight tracking, error, alpha, and plots)\n",
    "# For each iteration: print iteration, misclassified indices, weights, alpha, and update weights.\n",
    "# Plot: iteration vs weighted error, iteration vs alpha.\n",
    "# Report: train/test accuracy, confusion matrix, interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf7e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part D: Sklearn AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=0.6)\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "acc_ada = accuracy_score(y_test, y_pred_ada)\n",
    "conf_ada = confusion_matrix(y_test, y_pred_ada)\n",
    "acc_ada, conf_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f2721",
   "metadata": {},
   "source": [
    "## Q2: UCI Heart Disease Dataset — AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e978592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A: Baseline Model (Decision Stump)\n",
    "from sklearn.datasets import load_heart_disease\n",
    "data = load_heart_disease()  # Replace with actual loading method\n",
    "X, y = data.data, data.target\n",
    "# Preprocess categorical features, scaling if needed\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "stump = DecisionTreeClassifier(max_depth=1)\n",
    "stump.fit(X_train, y_train)\n",
    "y_pred = stump.predict(X_test)\n",
    "accuracy_score(y_test, y_pred), confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a12caf1",
   "metadata": {},
   "source": [
    "# Part B: Train AdaBoost with different n_estimators and learning_rate, plot accuracy\n",
    "# Part C: Track sample weights and errors for best model, plot error and weight distribution\n",
    "# Part D: Visualize feature importance and discuss medically relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff736fed",
   "metadata": {},
   "source": [
    "## Q2: WISDM Smartphone & Watch Motion Sensor Dataset — AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation: load, extract accelerometer features, create binary label, handle missing, train-test split\n",
    "# Baseline: Decision stump\n",
    "# Manual AdaBoost: 20 rounds, print iteration, misclassified indices, weights\n",
    "# Sklearn AdaBoost: n_estimators=100, learning_rate=1.0, compare results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
